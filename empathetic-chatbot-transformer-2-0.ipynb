{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc599ed",
   "metadata": {
    "papermill": {
     "duration": 0.004635,
     "end_time": "2025-10-16T08:36:36.684033",
     "exception": false,
     "start_time": "2025-10-16T08:36:36.679398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d69b35",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-16T08:36:36.692765Z",
     "iopub.status.busy": "2025-10-16T08:36:36.692160Z",
     "iopub.status.idle": "2025-10-16T08:36:47.663113Z",
     "shell.execute_reply": "2025-10-16T08:36:47.662078Z"
    },
    "papermill": {
     "duration": 10.976676,
     "end_time": "2025-10-16T08:36:47.664498",
     "exception": false,
     "start_time": "2025-10-16T08:36:36.687822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m‚úì All dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages (removed nlpaug)\n",
    "!pip install tokenizers sacrebleu rouge-score streamlit -q\n",
    "\n",
    "print(\"‚úì All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b67c8c",
   "metadata": {
    "papermill": {
     "duration": 0.003726,
     "end_time": "2025-10-16T08:36:47.672490",
     "exception": false,
     "start_time": "2025-10-16T08:36:47.668764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import All Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34be880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T08:36:47.681717Z",
     "iopub.status.busy": "2025-10-16T08:36:47.680895Z",
     "iopub.status.idle": "2025-10-16T08:36:59.639117Z",
     "shell.execute_reply": "2025-10-16T08:36:59.638211Z"
    },
    "papermill": {
     "duration": 11.964064,
     "end_time": "2025-10-16T08:36:59.640316",
     "exception": false,
     "start_time": "2025-10-16T08:36:47.676252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ DEVICE SETUP\n",
      "======================================================================\n",
      "‚úì Using device: cuda\n",
      "‚úì GPU: Tesla T4\n",
      "‚úì Memory: 15.83 GB\n",
      "‚úì CUDA Version: 12.4\n",
      "======================================================================\n",
      "\n",
      "üéØ IMPROVEMENTS LOADED:\n",
      "   ‚úÖ Repetition penalty mechanism\n",
      "   ‚úÖ Nucleus sampling (Top-p)\n",
      "   ‚úÖ Extended training schedule support\n",
      "   ‚úÖ Multiple decoding strategies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Core PyTorch and Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenization\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Evaluation metrics\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# GPU setup - CUDA optimization for Kaggle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üöÄ DEVICE SETUP\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚úì Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"‚úì CUDA Version: {torch.version.cuda}\")\n",
    "    # Enable cuDNN benchmarking for faster training\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"‚ö† No GPU available - using CPU\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"üéØ IMPROVEMENTS LOADED:\")\n",
    "print(\"   ‚úÖ Repetition penalty mechanism\")\n",
    "print(\"   ‚úÖ Nucleus sampling (Top-p)\")\n",
    "print(\"   ‚úÖ Extended training schedule support\")\n",
    "print(\"   ‚úÖ Multiple decoding strategies\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14b753",
   "metadata": {
    "papermill": {
     "duration": 0.003719,
     "end_time": "2025-10-16T08:36:59.648291",
     "exception": false,
     "start_time": "2025-10-16T08:36:59.644572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Load Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24efda44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T08:36:59.657624Z",
     "iopub.status.busy": "2025-10-16T08:36:59.657236Z",
     "iopub.status.idle": "2025-10-16T08:37:07.622039Z",
     "shell.execute_reply": "2025-10-16T08:37:07.621049Z"
    },
    "papermill": {
     "duration": 7.970928,
     "end_time": "2025-10-16T08:37:07.623306",
     "exception": false,
     "start_time": "2025-10-16T08:36:59.652378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä DATASET LOADING & PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "‚úì Dataset loaded successfully!\n",
      "üìä Shape: (64636, 7)\n",
      "üß© Columns: ['Unnamed: 0', 'Situation', 'emotion', 'empathetic_dialogues', 'labels', 'Unnamed: 5', 'Unnamed: 6']\n",
      "\n",
      "üìã First few rows:\n",
      "   Unnamed: 0                                          Situation      emotion  \\\n",
      "0           0  I remember going to the fireworks with my best...  sentimental   \n",
      "1           1  I remember going to the fireworks with my best...  sentimental   \n",
      "2           2  I remember going to the fireworks with my best...  sentimental   \n",
      "\n",
      "                                empathetic_dialogues  \\\n",
      "0  Customer :I remember going to see the firework...   \n",
      "1  Customer :This was a best friend. I miss her.\\...   \n",
      "2              Customer :We no longer talk.\\nAgent :   \n",
      "\n",
      "                                              labels Unnamed: 5 Unnamed: 6  \n",
      "0  Was this a friend you were in love with, or ju...        NaN        NaN  \n",
      "1                                Where has she gone?        NaN        NaN  \n",
      "2  Oh was this something that happened because of...        NaN        NaN  \n",
      "\n",
      "======================================================================\n",
      "üìà DATASET OVERVIEW\n",
      "======================================================================\n",
      "Total conversations: 64636\n",
      "Unique situations: 19205\n",
      "Unique emotions: 43\n",
      "\n",
      "üé≠ Emotion distribution:\n",
      "emotion\n",
      "surprised    3295\n",
      "excited      2465\n",
      "angry        2296\n",
      "proud        2247\n",
      "annoyed      2213\n",
      "sad          2213\n",
      "lonely       2106\n",
      "afraid       2094\n",
      "grateful     2091\n",
      "terrified    2074\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "üîß PROCESSING DATA PAIRS (No Augmentation)\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64636/64636 [00:07<00:00, 8672.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Total valid pairs created: 64636\n",
      "‚ö† Skipped rows (missing data): 0\n",
      "\n",
      "======================================================================\n",
      "üìù SAMPLE INPUT-OUTPUT PAIRS\n",
      "======================================================================\n",
      "\n",
      "--- Example 1 ---\n",
      "INPUT: emotion : sentimental | situation : i remember going to the fireworks with my best friend . there was a lot of people , but it only felt like us in th...\n",
      "TARGET: was this a friend you were in love with , or just a best friend ?\n",
      "\n",
      "--- Example 2 ---\n",
      "INPUT: emotion : sentimental | situation : i remember going to the fireworks with my best friend . there was a lot of people , but it only felt like us in th...\n",
      "TARGET: where has she gone ?\n",
      "\n",
      "--- Example 3 ---\n",
      "INPUT: emotion : sentimental | situation : i remember going to the fireworks with my best friend . there was a lot of people , but it only felt like us in th...\n",
      "TARGET: oh was this something that happened because of an argument ?\n",
      "\n",
      "======================================================================\n",
      "üìä DATASET SPLIT (80-10-10)\n",
      "======================================================================\n",
      "‚úì Train: 51708 (80.0%)\n",
      "‚úì Validation: 6463 (10.0%)\n",
      "‚úì Test: 6465 (10.0%)\n",
      "‚úì Total: 64636\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*70}\")\n",
    "print(f\"üìä DATASET LOADING & PREPROCESSING\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/kaggle/input/empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv')\n",
    "\n",
    "print(f\"‚úì Dataset loaded successfully!\")\n",
    "print(f\"üìä Shape: {df.shape}\")\n",
    "print(f\"üß© Columns: {list(df.columns)}\\n\")\n",
    "\n",
    "# Display sample\n",
    "print(\"üìã First few rows:\")\n",
    "print(df.head(3))\n",
    "print()\n",
    "\n",
    "# Dataset overview\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üìà DATASET OVERVIEW\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total conversations: {len(df)}\")\n",
    "print(f\"Unique situations: {df['Situation'].nunique()}\")\n",
    "print(f\"Unique emotions: {df['emotion'].nunique()}\")\n",
    "print(f\"\\nüé≠ Emotion distribution:\")\n",
    "print(df['emotion'].value_counts().head(10))\n",
    "print()\n",
    "\n",
    "# Text normalization function\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text: lowercase, clean whitespace, normalize punctuation\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'([.!?,;:])', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Extract customer utterance from empathetic_dialogues\n",
    "def extract_customer_utterance(dialogue_text):\n",
    "    \"\"\"Extract the last customer utterance from dialogue\"\"\"\n",
    "    if pd.isna(dialogue_text):\n",
    "        return \"\"\n",
    "    \n",
    "    parts = re.split(r'(Customer :|Agent :)', str(dialogue_text))\n",
    "    customer_utterances = []\n",
    "    for i in range(len(parts)):\n",
    "        if parts[i].strip() == 'Customer :' and i + 1 < len(parts):\n",
    "            customer_utterances.append(parts[i + 1].strip())\n",
    "    \n",
    "    if customer_utterances:\n",
    "        last_utterance = customer_utterances[-1]\n",
    "        last_utterance = re.split(r'Agent :', last_utterance)[0].strip()\n",
    "        last_utterance = last_utterance.replace('\\\\n', ' ').strip()\n",
    "        return last_utterance\n",
    "    \n",
    "    return str(dialogue_text).strip()\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üîß PROCESSING DATA PAIRS (No Augmentation)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Create input-output pairs WITHOUT augmentation\n",
    "data_pairs = []\n",
    "skipped = 0\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "    emotion = row['emotion']\n",
    "    situation = row['Situation']\n",
    "    dialogue = row['empathetic_dialogues']\n",
    "    agent_reply = row['labels']\n",
    "    \n",
    "    if pd.isna(agent_reply) or pd.isna(situation):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    customer_utt = extract_customer_utterance(dialogue)\n",
    "    \n",
    "    if not customer_utt:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    # Create input-output pair\n",
    "    input_text = f\"emotion: {emotion} | situation: {situation} | customer: {customer_utt} agent:\"\n",
    "    target_text = agent_reply\n",
    "    \n",
    "    data_pairs.append({\n",
    "        'input': normalize_text(input_text),\n",
    "        'target': normalize_text(target_text),\n",
    "        'emotion': emotion\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úì Total valid pairs created: {len(data_pairs)}\")\n",
    "print(f\"‚ö† Skipped rows (missing data): {skipped}\")\n",
    "\n",
    "# Display sample pairs\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìù SAMPLE INPUT-OUTPUT PAIRS\")\n",
    "print(f\"{'='*70}\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"INPUT: {data_pairs[i]['input'][:150]}...\")\n",
    "    print(f\"TARGET: {data_pairs[i]['target']}\")\n",
    "\n",
    "# Split dataset: 80% train, 10% val, 10% test\n",
    "random.shuffle(data_pairs)\n",
    "total_size = len(data_pairs)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "\n",
    "train_data = data_pairs[:train_size]\n",
    "val_data = data_pairs[train_size:train_size + val_size]\n",
    "test_data = data_pairs[train_size + val_size:]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä DATASET SPLIT (80-10-10)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚úì Train: {len(train_data)} ({len(train_data)/total_size*100:.1f}%)\")\n",
    "print(f\"‚úì Validation: {len(val_data)} ({len(val_data)/total_size*100:.1f}%)\")\n",
    "print(f\"‚úì Test: {len(test_data)} ({len(test_data)/total_size*100:.1f}%)\")\n",
    "print(f\"‚úì Total: {total_size}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8ade3",
   "metadata": {
    "papermill": {
     "duration": 0.00677,
     "end_time": "2025-10-16T08:37:07.637484",
     "exception": false,
     "start_time": "2025-10-16T08:37:07.630714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build Tokenizer & Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ede576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T08:37:07.652422Z",
     "iopub.status.busy": "2025-10-16T08:37:07.652176Z",
     "iopub.status.idle": "2025-10-16T08:37:09.160615Z",
     "shell.execute_reply": "2025-10-16T08:37:09.159772Z"
    },
    "papermill": {
     "duration": 1.51761,
     "end_time": "2025-10-16T08:37:09.162001",
     "exception": false,
     "start_time": "2025-10-16T08:37:07.644391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üî§ TOKENIZER TRAINING (using 'tokenizers' library)\n",
      "======================================================================\n",
      "\n",
      "üîÑ Training BPE tokenizer on training data only...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‚úì Tokenizer trained successfully!\n",
      "‚úì Vocabulary size: 10000\n",
      "‚úì Special tokens:\n",
      "   - PAD: 0\n",
      "   - BOS: 1\n",
      "   - EOS: 2\n",
      "   - UNK: 3\n",
      "\n",
      "üìù Tokenizer test:\n",
      "   Input: 'i am feeling happy today'\n",
      "   Tokens: ['i', 'am', 'feeling', 'happy', 'today']\n",
      "   IDs: [49, 139, 535, 324, 362]\n",
      "\n",
      "======================================================================\n",
      "üì¶ CREATING PYTORCH DATASETS & DATALOADERS\n",
      "======================================================================\n",
      "\n",
      "‚úì Datasets created:\n",
      "   - Train batches: 1616 (batch_size=32)\n",
      "   - Val batches: 202\n",
      "   - Test batches: 203\n",
      "\n",
      "‚úì GPU memory pinning: Enabled\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*70}\")\n",
    "print(f\"üî§ TOKENIZER TRAINING (using 'tokenizers' library)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Create tokenizer using tokenizers library (Hugging Face)\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Prepare training texts (from TRAIN split only - as per requirements)\n",
    "train_texts = []\n",
    "for item in train_data:\n",
    "    train_texts.append(item['input'])\n",
    "    train_texts.append(item['target'])\n",
    "\n",
    "# Save to temporary file for tokenizer training\n",
    "with open('train_texts.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in train_texts:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "# Train BPE tokenizer with special tokens\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"],\n",
    "    vocab_size=10000,\n",
    "    min_frequency=2\n",
    ")\n",
    "\n",
    "print(\"üîÑ Training BPE tokenizer on training data only...\")\n",
    "tokenizer.train(['train_texts.txt'], trainer)\n",
    "tokenizer.save(\"empathetic_tokenizer.json\")\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "PAD_IDX = tokenizer.token_to_id(\"<pad>\")\n",
    "BOS_IDX = tokenizer.token_to_id(\"<bos>\")\n",
    "EOS_IDX = tokenizer.token_to_id(\"<eos>\")\n",
    "UNK_IDX = tokenizer.token_to_id(\"<unk>\")\n",
    "\n",
    "print(f\"\\n‚úì Tokenizer trained successfully!\")\n",
    "print(f\"‚úì Vocabulary size: {vocab_size}\")\n",
    "print(f\"‚úì Special tokens:\")\n",
    "print(f\"   - PAD: {PAD_IDX}\")\n",
    "print(f\"   - BOS: {BOS_IDX}\")\n",
    "print(f\"   - EOS: {EOS_IDX}\")\n",
    "print(f\"   - UNK: {UNK_IDX}\")\n",
    "\n",
    "# Test tokenizer\n",
    "test_text = \"i am feeling happy today\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(f\"\\nüìù Tokenizer test:\")\n",
    "print(f\"   Input: '{test_text}'\")\n",
    "print(f\"   Tokens: {encoded.tokens[:10]}\")\n",
    "print(f\"   IDs: {encoded.ids[:10]}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üì¶ CREATING PYTORCH DATASETS & DATALOADERS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# PyTorch Dataset class\n",
    "class EmpatheticDataset(Dataset):\n",
    "    def __init__(self, data_pairs, tokenizer, max_len=128):\n",
    "        self.data = data_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize input\n",
    "        input_enc = self.tokenizer.encode(item['input'])\n",
    "        input_ids = [BOS_IDX] + input_enc.ids[:self.max_len-2] + [EOS_IDX]\n",
    "        \n",
    "        # Tokenize target\n",
    "        target_enc = self.tokenizer.encode(item['target'])\n",
    "        target_ids = [BOS_IDX] + target_enc.ids[:self.max_len-2] + [EOS_IDX]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Collate function for batching with padding\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad sequences in batch to same length\"\"\"\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    target_ids = [item['target_ids'] for item in batch]\n",
    "    \n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=PAD_IDX)\n",
    "    target_ids_padded = pad_sequence(target_ids, batch_first=True, padding_value=PAD_IDX)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'target_ids': target_ids_padded,\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmpatheticDataset(train_data, tokenizer)\n",
    "val_dataset = EmpatheticDataset(val_data, tokenizer)\n",
    "test_dataset = EmpatheticDataset(test_data, tokenizer)\n",
    "\n",
    "# Create dataloaders with GPU pinning for faster transfer\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    num_workers=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"‚úì Datasets created:\")\n",
    "print(f\"   - Train batches: {len(train_loader)} (batch_size={BATCH_SIZE})\")\n",
    "print(f\"   - Val batches: {len(val_loader)}\")\n",
    "print(f\"   - Test batches: {len(test_loader)}\")\n",
    "print(f\"\\n‚úì GPU memory pinning: {'Enabled' if torch.cuda.is_available() else 'Disabled'}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb23c8f",
   "metadata": {
    "papermill": {
     "duration": 0.007017,
     "end_time": "2025-10-16T08:37:09.176533",
     "exception": false,
     "start_time": "2025-10-16T08:37:09.169516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Complete Transformer Model (Built from Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ea2786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T08:37:09.191719Z",
     "iopub.status.busy": "2025-10-16T08:37:09.191422Z",
     "iopub.status.idle": "2025-10-16T08:37:09.739398Z",
     "shell.execute_reply": "2025-10-16T08:37:09.738469Z"
    },
    "papermill": {
     "duration": 0.557372,
     "end_time": "2025-10-16T08:37:09.740990",
     "exception": false,
     "start_time": "2025-10-16T08:37:09.183618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üèóÔ∏è BUILDING TRANSFORMER MODEL FROM SCRATCH\n",
      "======================================================================\n",
      "\n",
      "üîß Initializing Transformer model...\n",
      "\n",
      "‚úì Model initialized successfully!\n",
      "\n",
      "üìä MODEL STATISTICS:\n",
      "   - Total parameters: 11,376,400\n",
      "   - Trainable parameters: 11,376,400\n",
      "   - Model size: ~43.40 MB\n",
      "\n",
      "üèóÔ∏è ARCHITECTURE:\n",
      "   - Encoder layers: 2\n",
      "   - Decoder layers: 2\n",
      "   - Attention heads per layer: 2\n",
      "   - Embedding dimension: 256\n",
      "   - Feed-forward dimension: 1024\n",
      "   - Vocabulary size: 10000\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*70}\")\n",
    "print(f\"üèóÔ∏è BUILDING TRANSFORMER MODEL FROM SCRATCH\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# POSITIONAL ENCODING\n",
    "# ============================================================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Adds positional information to embeddings using sin/cos functions\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# ============================================================================\n",
    "# MULTI-HEAD ATTENTION\n",
    "# ============================================================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head scaled dot-product attention mechanism\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "        \n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output, attn_probs\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_len, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output, attn_probs = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output, attn_probs\n",
    "\n",
    "# ============================================================================\n",
    "# POSITION-WISE FEED-FORWARD NETWORK\n",
    "# ============================================================================\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"Two-layer feed-forward network with ReLU activation\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "# ============================================================================\n",
    "# ENCODER LAYER\n",
    "# ============================================================================\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single Transformer encoder layer\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# ============================================================================\n",
    "# DECODER LAYER\n",
    "# ============================================================================\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single Transformer decoder layer\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE TRANSFORMER CHATBOT MODEL\n",
    "# ============================================================================\n",
    "class TransformerChatbot(nn.Module):\n",
    "    \"\"\"Complete Transformer Encoder-Decoder for empathetic dialogue generation\"\"\"\n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=2, num_encoder_layers=2, \n",
    "                 num_decoder_layers=2, d_ff=1024, max_seq_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Encoder stack (2 layers as per specification)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder stack (2 layers as per specification)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection to vocabulary\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def generate_mask(self, src, tgt):\n",
    "        \"\"\"Generate source and target masks\"\"\"\n",
    "        src_mask = (src != PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_pad_mask = (tgt != PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"Encode source sequence\"\"\"\n",
    "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, enc_output, src_mask, tgt_mask):\n",
    "        \"\"\"Decode target sequence\"\"\"\n",
    "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"Forward pass through encoder-decoder\"\"\"\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        enc_output = self.encode(src, src_mask)\n",
    "        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "        output = self.output_projection(dec_output)\n",
    "        return output\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE MODEL\n",
    "# ============================================================================\n",
    "print(\"üîß Initializing Transformer model...\")\n",
    "model = TransformerChatbot(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256,\n",
    "    num_heads=2,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úì Model initialized successfully!\")\n",
    "print(f\"\\nüìä MODEL STATISTICS:\")\n",
    "print(f\"   - Total parameters: {total_params:,}\")\n",
    "print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   - Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "print(f\"\\nüèóÔ∏è ARCHITECTURE:\")\n",
    "print(f\"   - Encoder layers: {model.num_encoder_layers}\")\n",
    "print(f\"   - Decoder layers: {model.num_decoder_layers}\")\n",
    "print(f\"   - Attention heads per layer: 2\")\n",
    "print(f\"   - Embedding dimension: {model.d_model}\")\n",
    "print(f\"   - Feed-forward dimension: 1024\")\n",
    "print(f\"   - Vocabulary size: {vocab_size}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca710c",
   "metadata": {
    "papermill": {
     "duration": 0.007356,
     "end_time": "2025-10-16T08:37:09.757249",
     "exception": false,
     "start_time": "2025-10-16T08:37:09.749893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Setup & Extended Training with Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8724e14e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T08:37:09.773538Z",
     "iopub.status.busy": "2025-10-16T08:37:09.773274Z",
     "iopub.status.idle": "2025-10-16T09:05:29.792694Z",
     "shell.execute_reply": "2025-10-16T09:05:29.791768Z"
    },
    "papermill": {
     "duration": 1700.029071,
     "end_time": "2025-10-16T09:05:29.793955",
     "exception": false,
     "start_time": "2025-10-16T08:37:09.764884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üéØ TRAINING SETUP WITH IMPROVEMENTS\n",
      "======================================================================\n",
      "\n",
      "‚úì Loss function: CrossEntropyLoss (ignore_index=0)\n",
      "‚úì Optimizer: Adam (lr=1e-4, betas=(0.9, 0.98))\n",
      "‚úì Scheduler: Noam (warmup=4000 steps)\n",
      "‚úì Tokenizer parallelism: Disabled (fixes multiprocessing conflicts)\n",
      "\n",
      "======================================================================\n",
      "üî¨ SANITY CHECK: Overfitting on 10 samples\n",
      "======================================================================\n",
      "\n",
      "Testing if model can learn (overfitting on 10 samples)...\n",
      "Epoch 10/50, Loss: 0.9199\n",
      "Epoch 20/50, Loss: 5.3793\n",
      "Epoch 30/50, Loss: 4.4935\n",
      "Epoch 40/50, Loss: 4.3679\n",
      "Epoch 50/50, Loss: 4.2310\n",
      "\n",
      "‚ö† Warning: Model struggling to overfit (final loss: 4.2310)\n",
      "\n",
      "======================================================================\n",
      "üöÄ FULL TRAINING WITH 3-PHASE SCHEDULE\n",
      "======================================================================\n",
      "\n",
      "üìÖ TRAINING SCHEDULE:\n",
      "   Phase 1: 10 epochs @ lr=0.0001 (Fast learning)\n",
      "   Phase 2: 10 epochs @ lr=5e-05 (Fine-tuning)\n",
      "   Phase 3: 5 epochs @ lr=1e-05 (Polish)\n",
      "   Total: 25 epochs\n",
      "\n",
      "üì¶ Recreating dataloaders (fixing multiprocessing)...\n",
      "‚úì Dataloaders recreated with num_workers=0\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üìç PHASE 1: FAST LEARNING (10 epochs)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Epoch 1/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:00<00:00, 26.56it/s, loss=4.6018, lr=0.000399]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 72.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 1 Results:\n",
      "   Train Loss: 5.3087 | Val Loss: 4.3849\n",
      "   Train Perplexity: 202.08 | Val Perplexity: 80.23\n",
      "   Learning Rate: 0.000399\n",
      "   ‚úì Best model saved! (Val Loss: 4.3849)\n",
      "\n",
      "======================================================================\n",
      "Epoch 2/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 24.93it/s, loss=4.4958, lr=0.000798]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 2 Results:\n",
      "   Train Loss: 4.2738 | Val Loss: 4.1814\n",
      "   Train Perplexity: 71.79 | Val Perplexity: 65.46\n",
      "   Learning Rate: 0.000798\n",
      "   ‚úì Best model saved! (Val Loss: 4.1814)\n",
      "\n",
      "======================================================================\n",
      "Epoch 3/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.10it/s, loss=3.9393, lr=0.000898]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 3 Results:\n",
      "   Train Loss: 4.1173 | Val Loss: 4.0777\n",
      "   Train Perplexity: 61.40 | Val Perplexity: 59.01\n",
      "   Learning Rate: 0.000898\n",
      "   ‚úì Best model saved! (Val Loss: 4.0777)\n",
      "\n",
      "======================================================================\n",
      "Epoch 4/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.05it/s, loss=3.7390, lr=0.000777]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 4 Results:\n",
      "   Train Loss: 3.9701 | Val Loss: 3.9988\n",
      "   Train Perplexity: 52.99 | Val Perplexity: 54.53\n",
      "   Learning Rate: 0.000777\n",
      "   ‚úì Best model saved! (Val Loss: 3.9988)\n",
      "\n",
      "======================================================================\n",
      "Epoch 5/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.02it/s, loss=3.9867, lr=0.000695]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 5 Results:\n",
      "   Train Loss: 3.8611 | Val Loss: 3.9205\n",
      "   Train Perplexity: 47.52 | Val Perplexity: 50.42\n",
      "   Learning Rate: 0.000695\n",
      "   ‚úì Best model saved! (Val Loss: 3.9205)\n",
      "\n",
      "======================================================================\n",
      "Epoch 6/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.06it/s, loss=3.5661, lr=0.000635]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 6 Results:\n",
      "   Train Loss: 3.7814 | Val Loss: 3.8695\n",
      "   Train Perplexity: 43.88 | Val Perplexity: 47.92\n",
      "   Learning Rate: 0.000635\n",
      "   ‚úì Best model saved! (Val Loss: 3.8695)\n",
      "\n",
      "======================================================================\n",
      "Epoch 7/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 24.99it/s, loss=3.8386, lr=0.000588]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 7 Results:\n",
      "   Train Loss: 3.7218 | Val Loss: 3.8542\n",
      "   Train Perplexity: 41.34 | Val Perplexity: 47.19\n",
      "   Learning Rate: 0.000588\n",
      "   ‚úì Best model saved! (Val Loss: 3.8542)\n",
      "\n",
      "======================================================================\n",
      "Epoch 8/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 24.97it/s, loss=3.6343, lr=0.000550]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 8 Results:\n",
      "   Train Loss: 3.6712 | Val Loss: 3.8343\n",
      "   Train Perplexity: 39.30 | Val Perplexity: 46.26\n",
      "   Learning Rate: 0.000550\n",
      "   ‚úì Best model saved! (Val Loss: 3.8343)\n",
      "\n",
      "======================================================================\n",
      "Epoch 9/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.00it/s, loss=3.8686, lr=0.000518]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 9 Results:\n",
      "   Train Loss: 3.6283 | Val Loss: 3.8256\n",
      "   Train Perplexity: 37.65 | Val Perplexity: 45.86\n",
      "   Learning Rate: 0.000518\n",
      "   ‚úì Best model saved! (Val Loss: 3.8256)\n",
      "\n",
      "======================================================================\n",
      "Epoch 10/25 (Phase 1)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.00it/s, loss=3.4545, lr=0.000492]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 10 Results:\n",
      "   Train Loss: 3.5907 | Val Loss: 3.8158\n",
      "   Train Perplexity: 36.26 | Val Perplexity: 45.41\n",
      "   Learning Rate: 0.000492\n",
      "   ‚úì Best model saved! (Val Loss: 3.8158)\n",
      "\n",
      "======================================================================\n",
      "üìç PHASE 2: FINE-TUNING (10 epochs)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Epoch 11/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 24.95it/s, loss=3.1210, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 11 Results:\n",
      "   Train Loss: 3.4409 | Val Loss: 3.7703\n",
      "   Train Perplexity: 31.21 | Val Perplexity: 43.39\n",
      "   Learning Rate: 0.000050\n",
      "   ‚úì Best model saved! (Val Loss: 3.7703)\n",
      "\n",
      "======================================================================\n",
      "Epoch 12/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.01it/s, loss=3.1786, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 69.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 12 Results:\n",
      "   Train Loss: 3.4070 | Val Loss: 3.7646\n",
      "   Train Perplexity: 30.17 | Val Perplexity: 43.15\n",
      "   Learning Rate: 0.000050\n",
      "   ‚úì Best model saved! (Val Loss: 3.7646)\n",
      "\n",
      "======================================================================\n",
      "Epoch 13/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.07it/s, loss=3.3742, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 13 Results:\n",
      "   Train Loss: 3.3912 | Val Loss: 3.7648\n",
      "   Train Perplexity: 29.70 | Val Perplexity: 43.15\n",
      "   Learning Rate: 0.000050\n",
      "\n",
      "======================================================================\n",
      "Epoch 14/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.02it/s, loss=3.4230, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 14 Results:\n",
      "   Train Loss: 3.3803 | Val Loss: 3.7660\n",
      "   Train Perplexity: 29.38 | Val Perplexity: 43.21\n",
      "   Learning Rate: 0.000050\n",
      "\n",
      "======================================================================\n",
      "Epoch 15/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.11it/s, loss=3.6025, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 15 Results:\n",
      "   Train Loss: 3.3710 | Val Loss: 3.7702\n",
      "   Train Perplexity: 29.11 | Val Perplexity: 43.39\n",
      "   Learning Rate: 0.000050\n",
      "\n",
      "======================================================================\n",
      "Epoch 16/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.01it/s, loss=3.1408, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 16 Results:\n",
      "   Train Loss: 3.3631 | Val Loss: 3.7696\n",
      "   Train Perplexity: 28.88 | Val Perplexity: 43.36\n",
      "   Learning Rate: 0.000050\n",
      "\n",
      "======================================================================\n",
      "Epoch 17/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.00it/s, loss=2.9445, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 17 Results:\n",
      "   Train Loss: 3.3557 | Val Loss: 3.7732\n",
      "   Train Perplexity: 28.66 | Val Perplexity: 43.52\n",
      "   Learning Rate: 0.000050\n",
      "\n",
      "======================================================================\n",
      "Epoch 18/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.09it/s, loss=3.4410, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 18 Results:\n",
      "   Train Loss: 3.3483 | Val Loss: 3.7730\n",
      "   Train Perplexity: 28.46 | Val Perplexity: 43.51\n",
      "   Learning Rate: 0.000050\n",
      "\n",
      "======================================================================\n",
      "Epoch 19/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.06it/s, loss=3.5424, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 19 Results:\n",
      "   Train Loss: 3.3432 | Val Loss: 3.7759\n",
      "   Train Perplexity: 28.31 | Val Perplexity: 43.64\n",
      "   Learning Rate: 0.000050\n",
      "\n",
      "======================================================================\n",
      "Epoch 20/25 (Phase 2)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.01it/s, loss=3.1443, lr=0.000050]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 20 Results:\n",
      "   Train Loss: 3.3364 | Val Loss: 3.7760\n",
      "   Train Perplexity: 28.12 | Val Perplexity: 43.64\n",
      "   Learning Rate: 0.000050\n",
      "\n",
      "======================================================================\n",
      "üìç PHASE 3: POLISH (5 epochs)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Epoch 21/25 (Phase 3)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.05it/s, loss=3.2360, lr=0.000010]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 21 Results:\n",
      "   Train Loss: 3.3157 | Val Loss: 3.7763\n",
      "   Train Perplexity: 27.54 | Val Perplexity: 43.66\n",
      "   Learning Rate: 0.000010\n",
      "\n",
      "======================================================================\n",
      "Epoch 22/25 (Phase 3)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.05it/s, loss=3.0592, lr=0.000010]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 22 Results:\n",
      "   Train Loss: 3.3123 | Val Loss: 3.7756\n",
      "   Train Perplexity: 27.45 | Val Perplexity: 43.63\n",
      "   Learning Rate: 0.000010\n",
      "\n",
      "======================================================================\n",
      "Epoch 23/25 (Phase 3)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.01it/s, loss=3.5293, lr=0.000010]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 23 Results:\n",
      "   Train Loss: 3.3123 | Val Loss: 3.7765\n",
      "   Train Perplexity: 27.45 | Val Perplexity: 43.66\n",
      "   Learning Rate: 0.000010\n",
      "\n",
      "======================================================================\n",
      "Epoch 24/25 (Phase 3)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 24.99it/s, loss=3.3390, lr=0.000010]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 69.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 24 Results:\n",
      "   Train Loss: 3.3100 | Val Loss: 3.7768\n",
      "   Train Perplexity: 27.39 | Val Perplexity: 43.68\n",
      "   Learning Rate: 0.000010\n",
      "\n",
      "======================================================================\n",
      "Epoch 25/25 (Phase 3)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1616/1616 [01:04<00:00, 25.04it/s, loss=3.5682, lr=0.000010]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:02<00:00, 70.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 25 Results:\n",
      "   Train Loss: 3.3082 | Val Loss: 3.7771\n",
      "   Train Perplexity: 27.34 | Val Perplexity: 43.69\n",
      "   Learning Rate: 0.000010\n",
      "\n",
      "======================================================================\n",
      "‚úì TRAINING COMPLETE!\n",
      "======================================================================\n",
      "Total epochs trained: 25\n",
      "Best validation loss: 3.7646\n",
      "Best validation perplexity: 43.15\n",
      "\n",
      "üì• Loading best model...\n",
      "‚úì Loaded best model from epoch 12 (Phase 2)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*70}\")\n",
    "print(f\"üéØ TRAINING SETUP WITH IMPROVEMENTS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1e-4,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9\n",
    ")\n",
    "\n",
    "# Noam learning rate scheduler\n",
    "class NoamScheduler:\n",
    "    \"\"\"Learning rate scheduler with warmup\"\"\"\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.d_model ** (-0.5) * min(\n",
    "            self.current_step ** (-0.5), \n",
    "            self.current_step * self.warmup_steps ** (-1.5)\n",
    "        )\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "scheduler = NoamScheduler(optimizer, d_model=256, warmup_steps=4000)\n",
    "\n",
    "print(f\"‚úì Loss function: CrossEntropyLoss (ignore_index={PAD_IDX})\")\n",
    "print(f\"‚úì Optimizer: Adam (lr=1e-4, betas=(0.9, 0.98))\")\n",
    "print(f\"‚úì Scheduler: Noam (warmup=4000 steps)\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIX TOKENIZER PARALLELISM WARNING\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(f\"‚úì Tokenizer parallelism: Disabled (fixes multiprocessing conflicts)\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, use_scheduler=True):\n",
    "    \"\"\"Train for one epoch with teacher forcing\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        src = batch['input_ids'].to(device, non_blocking=True)\n",
    "        tgt = batch['target_ids'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Teacher forcing\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if use_scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if use_scheduler:\n",
    "            current_lr = scheduler.get_lr()\n",
    "        else:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'lr': f'{current_lr:.6f}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION FUNCTION\n",
    "# ============================================================================\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            src = batch['input_ids'].to(device, non_blocking=True)\n",
    "            tgt = batch['target_ids'].to(device, non_blocking=True)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            output = model(src, tgt_input)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üî¨ SANITY CHECK: Overfitting on 10 samples\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Sanity check\n",
    "small_data = train_data[:10]\n",
    "small_dataset = EmpatheticDataset(small_data, tokenizer)\n",
    "small_loader = DataLoader(small_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "# Higher learning rate for sanity check\n",
    "sanity_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9)\n",
    "sanity_scheduler = NoamScheduler(sanity_optimizer, d_model=256, warmup_steps=100)\n",
    "\n",
    "print(\"Testing if model can learn (overfitting on 10 samples)...\")\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in small_loader:\n",
    "        src = batch['input_ids'].to(device)\n",
    "        tgt = batch['target_ids'].to(device)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        sanity_optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "        loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        sanity_optimizer.step()\n",
    "        sanity_scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(small_loader)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/50, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    if avg_loss < 0.5:\n",
    "        print(f\"‚úì Loss dropped below 0.5 at epoch {epoch+1}!\")\n",
    "        break\n",
    "\n",
    "if avg_loss < 1.0:\n",
    "    print(f\"\\n‚úì Sanity check PASSED! Model can learn (final loss: {avg_loss:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Warning: Model struggling to overfit (final loss: {avg_loss:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üöÄ FULL TRAINING WITH 3-PHASE SCHEDULE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Reinitialize model for full training\n",
    "model = TransformerChatbot(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256,\n",
    "    num_heads=2,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# MULTI-PHASE TRAINING SCHEDULE\n",
    "PHASE1_EPOCHS = 10\n",
    "PHASE1_LR = 1e-4\n",
    "\n",
    "PHASE2_EPOCHS = 10\n",
    "PHASE2_LR = 5e-5\n",
    "\n",
    "PHASE3_EPOCHS = 5\n",
    "PHASE3_LR = 1e-5\n",
    "\n",
    "print(f\"üìÖ TRAINING SCHEDULE:\")\n",
    "print(f\"   Phase 1: {PHASE1_EPOCHS} epochs @ lr={PHASE1_LR} (Fast learning)\")\n",
    "print(f\"   Phase 2: {PHASE2_EPOCHS} epochs @ lr={PHASE2_LR} (Fine-tuning)\")\n",
    "print(f\"   Phase 3: {PHASE3_EPOCHS} epochs @ lr={PHASE3_LR} (Polish)\")\n",
    "print(f\"   Total: {PHASE1_EPOCHS + PHASE2_EPOCHS + PHASE3_EPOCHS} epochs\\n\")\n",
    "\n",
    "# Recreate dataloaders WITHOUT num_workers\n",
    "print(\"üì¶ Recreating dataloaders (fixing multiprocessing)...\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    num_workers=0\n",
    ")\n",
    "print(\"‚úì Dataloaders recreated with num_workers=0\\n\")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], \n",
    "    'val_loss': [], \n",
    "    'train_ppl': [], \n",
    "    'val_ppl': [],\n",
    "    'learning_rates': [],\n",
    "    'phases': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "all_epochs = PHASE1_EPOCHS + PHASE2_EPOCHS + PHASE3_EPOCHS\n",
    "current_epoch = 0\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: FAST LEARNING\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìç PHASE 1: FAST LEARNING ({PHASE1_EPOCHS} epochs)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=PHASE1_LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = NoamScheduler(optimizer, d_model=256, warmup_steps=4000)\n",
    "\n",
    "for epoch in range(PHASE1_EPOCHS):\n",
    "    current_epoch += 1\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {current_epoch}/{all_epochs} (Phase 1)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, device, use_scheduler=True)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    train_ppl = math.exp(min(train_loss, 10))\n",
    "    val_ppl = math.exp(min(val_loss, 10))\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['val_ppl'].append(val_ppl)\n",
    "    history['learning_rates'].append(scheduler.get_lr())\n",
    "    history['phases'].append('Phase 1')\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nüìä Epoch {current_epoch} Results:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"   Train Perplexity: {train_ppl:.2f} | Val Perplexity: {val_ppl:.2f}\")\n",
    "    print(f\"   Learning Rate: {scheduler.get_lr():.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': current_epoch,\n",
    "            'phase': 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "        }, 'best_model.pt')\n",
    "        print(f\"   ‚úì Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: FINE-TUNING\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìç PHASE 2: FINE-TUNING ({PHASE2_EPOCHS} epochs)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Reduce learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=PHASE2_LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "for epoch in range(PHASE2_EPOCHS):\n",
    "    current_epoch += 1\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {current_epoch}/{all_epochs} (Phase 2)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, None, device, use_scheduler=False)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_ppl = math.exp(min(train_loss, 10))\n",
    "    val_ppl = math.exp(min(val_loss, 10))\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['val_ppl'].append(val_ppl)\n",
    "    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "    history['phases'].append('Phase 2')\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {current_epoch} Results:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"   Train Perplexity: {train_ppl:.2f} | Val Perplexity: {val_ppl:.2f}\")\n",
    "    print(f\"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': current_epoch,\n",
    "            'phase': 2,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "        }, 'best_model.pt')\n",
    "        print(f\"   ‚úì Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: POLISH\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìç PHASE 3: POLISH ({PHASE3_EPOCHS} epochs)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Further reduce learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=PHASE3_LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "for epoch in range(PHASE3_EPOCHS):\n",
    "    current_epoch += 1\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {current_epoch}/{all_epochs} (Phase 3)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, None, device, use_scheduler=False)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_ppl = math.exp(min(train_loss, 10))\n",
    "    val_ppl = math.exp(min(val_loss, 10))\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['val_ppl'].append(val_ppl)\n",
    "    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "    history['phases'].append('Phase 3')\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {current_epoch} Results:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"   Train Perplexity: {train_ppl:.2f} | Val Perplexity: {val_ppl:.2f}\")\n",
    "    print(f\"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': current_epoch,\n",
    "            'phase': 3,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "        }, 'best_model.pt')\n",
    "        print(f\"   ‚úì Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úì TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total epochs trained: {current_epoch}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best validation perplexity: {math.exp(best_val_loss):.2f}\")\n",
    "\n",
    "# Load best model\n",
    "print(f\"\\nüì• Loading best model...\")\n",
    "checkpoint = torch.load('best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úì Loaded best model from epoch {checkpoint['epoch']} (Phase {checkpoint['phase']})\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a5e57",
   "metadata": {
    "papermill": {
     "duration": 2.398816,
     "end_time": "2025-10-16T09:05:34.565704",
     "exception": false,
     "start_time": "2025-10-16T09:05:32.166888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference with Advanced Decoding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4557c62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T09:05:39.358767Z",
     "iopub.status.busy": "2025-10-16T09:05:39.358299Z",
     "iopub.status.idle": "2025-10-16T09:07:49.464533Z",
     "shell.execute_reply": "2025-10-16T09:07:49.463718Z"
    },
    "papermill": {
     "duration": 134.61319,
     "end_time": "2025-10-16T09:07:51.528508",
     "exception": false,
     "start_time": "2025-10-16T09:05:36.915318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîÆ ADVANCED INFERENCE FUNCTIONS\n",
      "======================================================================\n",
      "\n",
      "‚úì Greedy decoding implemented\n",
      "‚úì Greedy with repetition penalty implemented (NEW)\n",
      "‚úì Nucleus sampling implemented (NEW)\n",
      "‚úì Beam search decoding implemented\n",
      "\n",
      "======================================================================\n",
      "üìä EVALUATION METRICS\n",
      "======================================================================\n",
      "\n",
      "‚úì BLEU metric ready\n",
      "‚úì ROUGE-L metric ready\n",
      "‚úì chrF metric ready\n",
      "‚úì Perplexity metric ready\n",
      "\n",
      "======================================================================\n",
      "üß™ RUNNING EVALUATION ON TEST SET\n",
      "======================================================================\n",
      "\n",
      "üìä Comparing different decoding strategies...\n",
      "\n",
      "Testing greedy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (greedy):   7%|‚ñã         | 15/203 [00:19<04:03,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing greedy_penalty...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (greedy_penalty):   7%|‚ñã         | 15/203 [00:18<03:50,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing nucleus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (nucleus):   7%|‚ñã         | 15/203 [00:26<05:34,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing beam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (beam):   7%|‚ñã         | 15/203 [01:03<13:20,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìà TEST SET RESULTS COMPARISON\n",
      "======================================================================\n",
      "Method               BLEU         ROUGE-L      chrF         Perplexity  \n",
      "--------------------------------------------------------------------\n",
      "greedy               2.60         15.91        12.59        1.35        \n",
      "greedy_penalty       2.40         16.02        12.26        1.35        \n",
      "nucleus              1.26         11.63        13.76        1.35        \n",
      "beam                 2.69         15.49        11.33        1.35        \n",
      "======================================================================\n",
      "\n",
      "‚úì Best performing method: beam\n",
      "‚úì Best BLEU score: 2.69\n",
      "\n",
      "======================================================================\n",
      "üìù QUALITATIVE EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "--- Example 1 ---\n",
      "Reference: no , and it gets progress ive ly more expensive .\n",
      "Generated: i ' m not sure yet . i ' ll be able to do that .\n",
      "\n",
      "--- Example 2 ---\n",
      "Reference: : ) we have all being through the age . you will miss this stage when they finally grow to become adult\n",
      "Generated: i ' m not sure , but i ' m not sure if i ' ll do it .\n",
      "\n",
      "--- Example 3 ---\n",
      "Reference: why is that ?\n",
      "Generated: i ' m sure you ' ll be fine !\n",
      "\n",
      "--- Example 4 ---\n",
      "Reference: did it escape your house ?\n",
      "Generated: oh no , i ' m sorry to hear that .\n",
      "\n",
      "--- Example 5 ---\n",
      "Reference: ah , so he had ul ter ior mot ives and wanted the chores done ! just kidding . glad you ' re feeling better .\n",
      "Generated: that ' s good to hear . i ' m glad he ' m glad he ' s okay !\n",
      "\n",
      "======================================================================\n",
      "üé≠ CUSTOM TEST INPUTS WITH ALL METHODS\n",
      "======================================================================\n",
      "\n",
      "Input: emotion: sad | situation: my dog passed away last week | customer: i miss him so...\n",
      "Greedy:         i ' m sorry to hear that. i ' m sorry to hear that. i hope you find him.\n",
      "Greedy+Penalty: i ' m sorry to hear that.\n",
      "Nucleus (p=0.9): what happened?\n",
      "Beam (width=3): i ' m so sorry to hear that.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Input: emotion: excited | situation: i got accepted to my dream university | customer: ...\n",
      "Greedy:         that ' s awesome! what are you going to do?\n",
      "Greedy+Penalty: that ' s awesome! what are you going to do?\n",
      "Nucleus (p=0.9): well that ' s good. i ' m sure you did not want to have a big puts your sake in your life.\n",
      "Beam (width=3): that ' s awesome! what are you going to do?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Input: emotion: afraid | situation: i have to give a presentation tomorrow | customer: ...\n",
      "Greedy:         i ' m sure you will do fine.\n",
      "Greedy+Penalty: i ' m sure you will do fine.\n",
      "Nucleus (p=0.9): why is that?\n",
      "Beam (width=3): i ' m sure you ' ll do great!\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "üíæ SAVING MODEL AND ARTIFACTS\n",
      "======================================================================\n",
      "\n",
      "‚úì Model saved: final_model.pt (includes all test results)\n",
      "‚úì Best checkpoint: best_model.pt (saved during training)\n",
      "‚úì Tokenizer saved: empathetic_tokenizer.json\n",
      "‚úì Evaluation results saved: evaluation_results.json\n",
      "‚úì Training history saved: training_history.json\n",
      "\n",
      "======================================================================\n",
      "üìÑ CREATING DEPLOYMENT FILES\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*70}\")\n",
    "print(f\"üîÆ ADVANCED INFERENCE FUNCTIONS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# GREEDY DECODING (Basic)\n",
    "# ============================================================================\n",
    "def greedy_decode(model, src, max_len=50):\n",
    "    \"\"\"Standard greedy decoding\"\"\"\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_mask, _ = model.generate_mask(src, src)\n",
    "        enc_output = model.encode(src, src_mask)\n",
    "        tgt = torch.tensor([[BOS_IDX]], device=device)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            _, tgt_mask = model.generate_mask(src, tgt)\n",
    "            dec_output = model.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "            output = model.output_projection(dec_output[:, -1, :])\n",
    "            next_token = output.argmax(dim=-1, keepdim=True)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "            if next_token.item() == EOS_IDX:\n",
    "                break\n",
    "        \n",
    "        return tgt.squeeze(0).cpu().tolist()\n",
    "\n",
    "# ============================================================================\n",
    "# GREEDY DECODING WITH REPETITION PENALTY (NEW IMPROVEMENT)\n",
    "# ============================================================================\n",
    "def greedy_decode_with_penalty(model, src, max_len=50, repetition_penalty=1.2):\n",
    "    \"\"\"\n",
    "    Greedy decoding with repetition penalty to reduce repetitive outputs\n",
    "    Higher penalty (>1.0) discourages repeating tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_mask, _ = model.generate_mask(src, src)\n",
    "        enc_output = model.encode(src, src_mask)\n",
    "        tgt = torch.tensor([[BOS_IDX]], device=device)\n",
    "        generated_tokens = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            _, tgt_mask = model.generate_mask(src, tgt)\n",
    "            dec_output = model.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "            logits = model.output_projection(dec_output[:, -1, :]).squeeze(0)\n",
    "            \n",
    "            # Apply repetition penalty (NEW IMPROVEMENT)\n",
    "            for token in set(generated_tokens):\n",
    "                logits[token] = logits[token] / repetition_penalty\n",
    "            \n",
    "            next_token = logits.argmax(dim=-1, keepdim=True)\n",
    "            tgt = torch.cat([tgt, next_token.unsqueeze(0)], dim=1)\n",
    "            generated_tokens.append(next_token.item())\n",
    "            \n",
    "            if next_token.item() == EOS_IDX:\n",
    "                break\n",
    "        \n",
    "        return tgt.squeeze(0).cpu().tolist()\n",
    "\n",
    "# ============================================================================\n",
    "# NUCLEUS SAMPLING / TOP-P SAMPLING (NEW IMPROVEMENT)\n",
    "# ============================================================================\n",
    "def nucleus_sampling_decode(model, src, max_len=50, p=0.9, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Nucleus (top-p) sampling for more diverse and natural responses\n",
    "    p: cumulative probability threshold (0.9 = top 90% probability mass)\n",
    "    temperature: controls randomness (higher = more random)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_mask, _ = model.generate_mask(src, src)\n",
    "        enc_output = model.encode(src, src_mask)\n",
    "        tgt = torch.tensor([[BOS_IDX]], device=device)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            _, tgt_mask = model.generate_mask(src, tgt)\n",
    "            dec_output = model.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "            logits = model.output_projection(dec_output[:, -1, :]).squeeze(0)\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Sort by probability\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            # Remove tokens after cumulative probability exceeds p\n",
    "            sorted_indices_to_remove = cumulative_probs > p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            # Set removed tokens to -inf\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[indices_to_remove] = -float('Inf')\n",
    "            \n",
    "            # Sample from remaining distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            tgt = torch.cat([tgt, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            if next_token.item() == EOS_IDX:\n",
    "                break\n",
    "        \n",
    "        return tgt.squeeze(0).cpu().tolist()\n",
    "\n",
    "# ============================================================================\n",
    "# BEAM SEARCH DECODING\n",
    "# ============================================================================\n",
    "def beam_search_decode(model, src, beam_width=3, max_len=50):\n",
    "    \"\"\"Beam search decoding\"\"\"\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_mask, _ = model.generate_mask(src, src)\n",
    "        enc_output = model.encode(src, src_mask)\n",
    "        beams = [([BOS_IDX], 0.0)]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            candidates = []\n",
    "            \n",
    "            for seq, score in beams:\n",
    "                if seq[-1] == EOS_IDX:\n",
    "                    candidates.append((seq, score))\n",
    "                    continue\n",
    "                \n",
    "                tgt = torch.tensor([seq], device=device)\n",
    "                _, tgt_mask = model.generate_mask(src, tgt)\n",
    "                dec_output = model.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "                output = model.output_projection(dec_output[:, -1, :])\n",
    "                \n",
    "                log_probs = F.log_softmax(output, dim=-1)\n",
    "                top_probs, top_indices = log_probs.topk(beam_width)\n",
    "                \n",
    "                for prob, idx in zip(top_probs[0], top_indices[0]):\n",
    "                    candidates.append((seq + [idx.item()], score + prob.item()))\n",
    "            \n",
    "            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            \n",
    "            if all(seq[-1] == EOS_IDX for seq, _ in beams):\n",
    "                break\n",
    "        \n",
    "        return beams[0][0]\n",
    "\n",
    "# ============================================================================\n",
    "# UNIFIED GENERATE RESPONSE FUNCTION\n",
    "# ============================================================================\n",
    "def generate_response(model, input_text, method='greedy', **kwargs):\n",
    "    \"\"\"\n",
    "    Generate response with multiple decoding strategies\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        input_text: Input text string\n",
    "        method: 'greedy', 'greedy_penalty', 'nucleus', 'beam'\n",
    "        **kwargs: Additional parameters for each method\n",
    "            - repetition_penalty: for 'greedy_penalty' (default: 1.2)\n",
    "            - p: for 'nucleus' (default: 0.9)\n",
    "            - temperature: for 'nucleus' (default: 1.0)\n",
    "            - beam_width: for 'beam' (default: 3)\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    input_enc = tokenizer.encode(normalize_text(input_text))\n",
    "    input_ids = [BOS_IDX] + input_enc.ids + [EOS_IDX]\n",
    "    src = torch.tensor([input_ids], dtype=torch.long)\n",
    "    \n",
    "    # Generate based on method\n",
    "    if method == 'greedy':\n",
    "        output_ids = greedy_decode(model, src)\n",
    "    elif method == 'greedy_penalty':\n",
    "        repetition_penalty = kwargs.get('repetition_penalty', 1.2)\n",
    "        output_ids = greedy_decode_with_penalty(model, src, repetition_penalty=repetition_penalty)\n",
    "    elif method == 'nucleus':\n",
    "        p = kwargs.get('p', 0.9)\n",
    "        temperature = kwargs.get('temperature', 1.0)\n",
    "        output_ids = nucleus_sampling_decode(model, src, p=p, temperature=temperature)\n",
    "    elif method == 'beam':\n",
    "        beam_width = kwargs.get('beam_width', 3)\n",
    "        output_ids = beam_search_decode(model, src, beam_width=beam_width)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Decode output\n",
    "    output_tokens = [tokenizer.id_to_token(id) for id in output_ids \n",
    "                     if id not in [BOS_IDX, EOS_IDX, PAD_IDX]]\n",
    "    output_text = ' '.join(output_tokens)\n",
    "    output_text = output_text.replace(' ,', ',').replace(' .', '.').replace(' !', '!').replace(' ?', '?')\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "print(\"‚úì Greedy decoding implemented\")\n",
    "print(\"‚úì Greedy with repetition penalty implemented (NEW)\")\n",
    "print(\"‚úì Nucleus sampling implemented (NEW)\")\n",
    "print(\"‚úì Beam search decoding implemented\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä EVALUATION METRICS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION METRICS\n",
    "# ============================================================================\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    \"\"\"Calculate BLEU score\"\"\"\n",
    "    bleu = BLEU()\n",
    "    score = bleu.corpus_score(hypotheses, [references])\n",
    "    return score.score\n",
    "\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    \"\"\"Calculate ROUGE-L score\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, hyp)['rougeL'].fmeasure \n",
    "              for ref, hyp in zip(references, hypotheses)]\n",
    "    return np.mean(scores) * 100\n",
    "\n",
    "def calculate_chrf(references, hypotheses):\n",
    "    \"\"\"Calculate chrF score\"\"\"\n",
    "    chrf = CHRF()\n",
    "    score = chrf.corpus_score(hypotheses, [references])\n",
    "    return score.score\n",
    "\n",
    "# ============================================================================\n",
    "# FULL EVALUATION WITH DIFFERENT DECODING METHODS\n",
    "# ============================================================================\n",
    "def evaluate_model(model, dataloader, device, num_samples=500, method='greedy'):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    total_loss = 0\n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating ({method})\"):\n",
    "            src = batch['input_ids'].to(device, non_blocking=True)\n",
    "            tgt = batch['target_ids'].to(device, non_blocking=True)\n",
    "            \n",
    "            # Calculate loss\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            output = model(src, tgt_input)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Generate predictions\n",
    "            for i in range(src.size(0)):\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "                \n",
    "                # Generate prediction based on method\n",
    "                if method == 'greedy':\n",
    "                    pred_ids = greedy_decode(model, src[i:i+1])\n",
    "                elif method == 'greedy_penalty':\n",
    "                    pred_ids = greedy_decode_with_penalty(model, src[i:i+1], repetition_penalty=1.2)\n",
    "                elif method == 'nucleus':\n",
    "                    pred_ids = nucleus_sampling_decode(model, src[i:i+1], p=0.9)\n",
    "                elif method == 'beam':\n",
    "                    pred_ids = beam_search_decode(model, src[i:i+1], beam_width=3)\n",
    "                \n",
    "                # Get reference\n",
    "                ref_ids = tgt[i].cpu().tolist()\n",
    "                ref_tokens = [tokenizer.id_to_token(id) for id in ref_ids \n",
    "                             if id not in [BOS_IDX, EOS_IDX, PAD_IDX]]\n",
    "                pred_tokens = [tokenizer.id_to_token(id) for id in pred_ids \n",
    "                              if id not in [BOS_IDX, EOS_IDX, PAD_IDX]]\n",
    "                \n",
    "                ref_text = ' '.join(ref_tokens)\n",
    "                pred_text = ' '.join(pred_tokens)\n",
    "                \n",
    "                references.append(ref_text)\n",
    "                hypotheses.append(pred_text)\n",
    "                \n",
    "                sample_count += 1\n",
    "            \n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    perplexity = math.exp(total_loss / len(dataloader))\n",
    "    bleu_score = calculate_bleu(references, hypotheses)\n",
    "    rouge_score = calculate_rouge(references, hypotheses)\n",
    "    chrf_score = calculate_chrf(references, hypotheses)\n",
    "    \n",
    "    return {\n",
    "        'perplexity': perplexity,\n",
    "        'bleu': bleu_score,\n",
    "        'rouge_l': rouge_score,\n",
    "        'chrf': chrf_score,\n",
    "        'references': references,\n",
    "        'hypotheses': hypotheses\n",
    "    }\n",
    "\n",
    "print(\"‚úì BLEU metric ready\")\n",
    "print(\"‚úì ROUGE-L metric ready\")\n",
    "print(\"‚úì chrF metric ready\")\n",
    "print(\"‚úì Perplexity metric ready\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üß™ RUNNING EVALUATION ON TEST SET\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Evaluate with different methods\n",
    "print(\"üìä Comparing different decoding strategies...\\n\")\n",
    "\n",
    "results_comparison = {}\n",
    "\n",
    "for method_name in ['greedy', 'greedy_penalty', 'nucleus', 'beam']:\n",
    "    print(f\"Testing {method_name}...\")\n",
    "    results = evaluate_model(model, test_loader, device, num_samples=500, method=method_name)\n",
    "    results_comparison[method_name] = results\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìà TEST SET RESULTS COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Method':<20} {'BLEU':<12} {'ROUGE-L':<12} {'chrF':<12} {'Perplexity':<12}\")\n",
    "print(f\"{'-'*68}\")\n",
    "\n",
    "for method_name, results in results_comparison.items():\n",
    "    print(f\"{method_name:<20} {results['bleu']:<12.2f} {results['rouge_l']:<12.2f} \"\n",
    "          f\"{results['chrf']:<12.2f} {results['perplexity']:<12.2f}\")\n",
    "\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Use best performing method for final results\n",
    "best_method = max(results_comparison.items(), key=lambda x: x[1]['bleu'])\n",
    "test_results = best_method[1]\n",
    "print(f\"‚úì Best performing method: {best_method[0]}\")\n",
    "print(f\"‚úì Best BLEU score: {test_results['bleu']:.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìù QUALITATIVE EXAMPLES\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Show 5 random examples\n",
    "for i in range(5):\n",
    "    idx = random.randint(0, len(test_results['references'])-1)\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(f\"Reference: {test_results['references'][idx]}\")\n",
    "    print(f\"Generated: {test_results['hypotheses'][idx]}\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üé≠ CUSTOM TEST INPUTS WITH ALL METHODS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Test with custom inputs\n",
    "test_inputs = [\n",
    "    \"emotion: sad | situation: my dog passed away last week | customer: i miss him so much agent:\",\n",
    "    \"emotion: excited | situation: i got accepted to my dream university | customer: i can't believe it happened! agent:\",\n",
    "    \"emotion: afraid | situation: i have to give a presentation tomorrow | customer: i'm so nervous about speaking in public agent:\",\n",
    "]\n",
    "\n",
    "for inp in test_inputs:\n",
    "    print(f\"Input: {inp[:80]}...\")\n",
    "    print(f\"Greedy:         {generate_response(model, inp, method='greedy')}\")\n",
    "    print(f\"Greedy+Penalty: {generate_response(model, inp, method='greedy_penalty', repetition_penalty=1.2)}\")\n",
    "    print(f\"Nucleus (p=0.9): {generate_response(model, inp, method='nucleus', p=0.9, temperature=1.0)}\")\n",
    "    print(f\"Beam (width=3): {generate_response(model, inp, method='beam', beam_width=3)}\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üíæ SAVING MODEL AND ARTIFACTS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Save final model with best results\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'model_config': {\n",
    "        'd_model': 256,\n",
    "        'num_heads': 2,\n",
    "        'num_encoder_layers': 2,\n",
    "        'num_decoder_layers': 2,\n",
    "        'd_ff': 1024,\n",
    "        'dropout': 0.1\n",
    "    },\n",
    "    'test_results': {\n",
    "        'perplexity': test_results['perplexity'],\n",
    "        'bleu': test_results['bleu'],\n",
    "        'rouge_l': test_results['rouge_l'],\n",
    "        'chrf': test_results['chrf'],\n",
    "        'best_method': best_method[0]\n",
    "    },\n",
    "    'results_comparison': {k: {\n",
    "        'bleu': v['bleu'],\n",
    "        'rouge_l': v['rouge_l'],\n",
    "        'chrf': v['chrf'],\n",
    "        'perplexity': v['perplexity']\n",
    "    } for k, v in results_comparison.items()}\n",
    "}, 'final_model.pt')\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save(\"empathetic_tokenizer.json\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'best_method': best_method[0],\n",
    "        'perplexity': test_results['perplexity'],\n",
    "        'bleu': test_results['bleu'],\n",
    "        'rouge_l': test_results['rouge_l'],\n",
    "        'chrf': test_results['chrf'],\n",
    "        'all_methods': {k: {\n",
    "            'bleu': v['bleu'],\n",
    "            'rouge_l': v['rouge_l'],\n",
    "            'chrf': v['chrf'],\n",
    "            'perplexity': v['perplexity']\n",
    "        } for k, v in results_comparison.items()},\n",
    "        'num_samples': len(test_results['references'])\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"‚úì Model saved: final_model.pt (includes all test results)\")\n",
    "print(\"‚úì Best checkpoint: best_model.pt (saved during training)\")\n",
    "print(\"‚úì Tokenizer saved: empathetic_tokenizer.json\")\n",
    "print(\"‚úì Evaluation results saved: evaluation_results.json\")\n",
    "print(\"‚úì Training history saved: training_history.json\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìÑ CREATING DEPLOYMENT FILES\")\n",
    "print(f\"{'='*70}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1962861,
     "sourceId": 3238154,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1885.593623,
   "end_time": "2025-10-16T09:07:57.118816",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-16T08:36:31.525193",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
